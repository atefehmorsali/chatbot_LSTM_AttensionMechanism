{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot\n",
    "\n",
    "### Use bidirectional LSTM and attention mechanism \n",
    "### Dataset: [Movie Dialogue Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "movie_lines = open('dataset/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "movie_conversations = open('dataset/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_conversations[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def generate_quesAns(file_dir): \n",
    "    '''create a list of questions (inputs) and answers (targets) from data'''\n",
    "    \n",
    "    movie_dir = os.path.join(file_dir, 'movie_lines.txt')\n",
    "    convs_dir = os.path.join(file_dir, 'movie_conversations.txt' )\n",
    "    movie_lines = open(movie_dir, encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    movie_conversations = open(convs_dir, encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    \n",
    "    id_line = {}\n",
    "    convs_ids = [ ]\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    # a dictionary mapping line_ids and its corresponding text\n",
    "    for line in movie_lines:\n",
    "        txt = line.split(' +++$+++ ')\n",
    "        if len(txt) == 5:\n",
    "            id_line[txt[0]] = txt[4]\n",
    "    \n",
    "    # check the id_line dict\n",
    "    dict_pairs = id_line.items()\n",
    "    pairs_iterator = iter(dict_pairs)\n",
    "    first_pair = next(pairs_iterator)\n",
    "    print(f'first key_value of id_line dictionary: {first_pair}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # a list containing all the conversation line_ids\n",
    "    for line in movie_conversations[:-1]:\n",
    "        ids = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs_ids.append(ids.split(','))\n",
    "        \n",
    "    # check the convs_ids\n",
    "    print(f'the first two line of convs_ids: {convs_ids[:1]}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create questions and answers given the list of convs_ids and the sentence corresponding to each id\n",
    "    for conv_id in convs_ids:\n",
    "        for i in range(len(conv_id)-1):\n",
    "            questions.append(id_line[conv_id[i]])\n",
    "            answers.append(id_line[conv_id[i+1]])\n",
    "            \n",
    "    return questions, answers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first key_value of id_line dictionary: ('L1045', 'They do not!')\n",
      "the first two line of convs_ids: [['L194', 'L195', 'L196', 'L197']]\n"
     ]
    }
   ],
   "source": [
    "base_dir = './dataset'\n",
    "questions, answers = generate_quesAns(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(questions): 221616 & len(answers): 221616\n",
      "\n",
      "question0: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "answer0: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "question1: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "answer1: Not the hacking and gagging and spitting part.  Please.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'len(questions): {len(questions)} & len(answers): {len(answers)}\\n')\n",
    "\n",
    "for i in range(2):\n",
    "    print(f'question{i}: {questions[i]}')\n",
    "    print(f'answer{i}: {answers[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! sudo apt install openjdk-8-jdk\n",
    "# ! sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "# ! pip install language-check\n",
    "# ! pip install pycontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "# Choose model accordingly for contractions function\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "# model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "def clean_data_1(text):\n",
    "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [clean_data_1(ques) for ques in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [clean_data_1(ans) for ans in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question0: can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "answer0: well i thought we would start with pronunciation if that is okay with you\n",
      "\n",
      "question1: well i thought we would start with pronunciation if that is okay with you\n",
      "answer1: not the hacking and gagging and spitting part  please\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(f'question{i}: {questions[i]}')\n",
    "    print(f'answer{i}: {answers[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the data to be sure that it has been correctly formated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep questions and answers within min and max # of words, and remove those shorter than min_words and \n",
    "#longer than max_words'''\n",
    "temp_questions = []\n",
    "temp_answers = []\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "min_words = 2\n",
    "max_words = 20\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "#remove short/long questions\n",
    "for question in questions:  \n",
    "    length = len(question.split())\n",
    "    if  length>= min_words and length <= max_words:\n",
    "        temp_questions.append(question)\n",
    "        temp_answers.append(answers[i])\n",
    "    i += 1\n",
    "\n",
    "# remove short/long answers\n",
    "for answer in temp_answers:\n",
    "    length = len(answer.split())\n",
    "    if  length>= min_words and length <= max_words:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(temp_questions[j])\n",
    "    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138335 out of 221616 questions used\n",
      "138335 out of 221616 questions used\n",
      "62.4 % of data used\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(short_questions)} out of {len(questions)} questions used')\n",
    "print(f'{len(short_answers)} out of {len(answers)} questions used')\n",
    "print(f'{100*round(len(short_questions)/len(questions), 3)} % of data used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(threshold, questions, answers):\n",
    "    '''create a vocabulary dictionary representing frequency of each word in corpus'''\n",
    "    '''then, remove words with counts less than threshold from vocabulary '''\n",
    "    '''then, map each word in vocabulary to an integer'''\n",
    "    \n",
    "    tokens = ['<PAD>','<UNK>','<GO>', '<EOS>']\n",
    "    vocabulary = {}\n",
    "    vocab2int = {}\n",
    "        \n",
    "    for question in questions:\n",
    "        for word in question.split():\n",
    "            vocabulary[word] = vocabulary.get(word, 0) + 1\n",
    "                \n",
    "    for answer in answers:\n",
    "        for word in answer.split():\n",
    "            vocabulary[word] = vocabulary.get(word, 0) + 1\n",
    "        answer += ' <EOS>'  #Add EOS token to the end of answer\n",
    "    \n",
    "    \n",
    "    num = 0\n",
    "    for key, value in vocabulary.items():\n",
    "        if value >= threshold:\n",
    "            vocab2int[key] = num\n",
    "            num += 1\n",
    "    \n",
    "    for tok in tokens:\n",
    "        vocab2int[tok] = len(vocab2int)+1\n",
    "        \n",
    "    \n",
    "    return vocab2int"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
